{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.reddit.com/r/LangChain/comments/13jd9wo/improving_the_quality_of_qa_with_pdf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_dotenv():\n",
    "    print(\"Cannot load .env file. Environment file is not exists or not readable\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    PyMuPDFLoader,\n",
    "    PyPDFLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PyPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1].lower()\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension '{ext}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]: \n",
    "    all_files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext.lower()}\"), recursive=True)\n",
    "            )\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext.upper()}\"), recursive=True)\n",
    "        )\n",
    "    \n",
    "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
    "    print(filtered_files)\n",
    "    # multiprocessing\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        # progress bar\n",
    "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
    "            # apply function to each item in the iterable in parallel, pool.imap_unordered(task, items)\n",
    "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
    "                results.extend(docs)\n",
    "                pbar.update()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Load environment variables\n",
    "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
    "source_directory = os.environ.get('SOURCE_DIRECTORY', 'src_documents')\n",
    "embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"src_documents\\\\DIALOGPT.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\"],\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len)\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "len(docs)\n",
    "# len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['src_documents\\\\DIALOGPT.pdf', 'src_documents\\\\DIALOGPT.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents:   0%|                              | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    documents = load_documents(source_directory, [])\n",
    "    print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents and split in chunks\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        exit(0)\n",
    "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from src_documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents:   0%|                              | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "texts = process_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_vectorstore_exist(persist_directory: str, embeddings: HuggingFaceEmbeddings) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    # db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    # if not db.get()['documents']:\n",
    "    #     return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import CHROMA_SETTINGS\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "    # Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS , path=persist_directory)\n",
    "\n",
    "    if does_vectorstore_exist(persist_directory, embeddings):\n",
    "        # Update and store locally vectorstore\n",
    "        print(f\"Appending to existing vectorstore at {persist_directory}\")\n",
    "        db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS, client=chroma_client)\n",
    "        collection = db.get()\n",
    "        texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n",
    "        print(f\"Creating embeddings. May take some minutes...\")\n",
    "        db.add_documents(texts)\n",
    "    else:\n",
    "        # Create and store locally vectorstore\n",
    "        print(\"Creating new vectorstore\")\n",
    "        texts = process_documents()\n",
    "        print(f\"Creating embeddings. May take some minutes...\")\n",
    "        db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS, client=chroma_client)\n",
    "    db.persist()\n",
    "    db = None\n",
    "\n",
    "    print(f\"Ingestion complete! You can now run privateGPT.py to query your documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to existing vectorstore at db\n",
      "Loading documents from source_documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new documents to load\n",
      "Loaded 0 new documents from source_documents\n",
      "Split into 0 chunks of text (max. 500 tokens each)\n",
      "Creating embeddings. May take some minutes...\n",
      "[]\n",
      "Ingestion complete! You can now run privateGPT.py to query your documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_document_qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
