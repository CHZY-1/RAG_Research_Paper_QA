{"docstore/metadata": {"f25cb767-4813-46e9-a13b-d065d7a43d4c": {"doc_hash": "436ea260cdf7a962e8528dc85c2e22ee8a870e1313d88116de41e2885557fbc9"}, "a4dbe7bd-c621-4d0f-9cf5-c606c5bdbeca": {"doc_hash": "65d59f99d02be5a5ef6142ea379b39e96b9d6148e5eb12501ef0c2e50f34dd7b"}, "f0c8b384-d483-40f3-a5e1-ef9403759988": {"doc_hash": "6f7378c1bdd90df040de6f913b6b67133178796971a12f18028d6666cbffd859"}, "cb84160c-0d61-4e72-acf1-3c958f0b6612": {"doc_hash": "0711fe5fe93036f2d553e69dfd86c557373653f3285eb1b3b7038f39b0b9964e"}, "e54a3d05-9f94-4788-ac98-306bacd5a7fd": {"doc_hash": "0821f0a3aafc4e069257c105cf4c8a18a85ffb8bba7c705e926043bf9f80a906"}, "42520c7e-d853-4163-8f41-f72a055edd7f": {"doc_hash": "7c6f64c5e1ff7a1cda7f29d9a00839faeabcad1f83938186f40adcb21d8adc81"}, "23ebbb42-b384-4b38-9f8e-02741508004f": {"doc_hash": "dd06801e4d7fd8a0e312d30f1b47f0d261e509ece9269b0d40b389fde870bc10"}, "f8f7bf79-1e5a-4777-9a7c-1fabb237d5c3": {"doc_hash": "bbda9a6da5da84c4b996f11336b37d21f5909d33c357377fd7b413384ed65fa9"}, "00295924-da03-4122-81ec-e61b1e3d11b3": {"doc_hash": "6781ba2615bc0ad6f3b3591e8d1a539d1206dade4685355673ca4fcacdc78a10"}, "755014fd-2cc7-47a3-be4c-f4cfd8218a57": {"doc_hash": "1529feaed5e6a9c9a4a08cc5483c1562f1f1c3c14a0e124664031a664c20a323"}, "93cca829-42b2-46ed-8b90-c0ed8e48cc26": {"doc_hash": "c843ccb4f8dc34692521cc3b39d708a782887e622ce82ec493d6d3b572a03222", "ref_doc_id": "f25cb767-4813-46e9-a13b-d065d7a43d4c"}, "a0c7cd52-3c8d-44f3-b3a7-a9143f3bc919": {"doc_hash": "6231c226b7cea277aee7317a5c4c3cf0d70c92d986ac0d7945ae46f1e8ef863f", "ref_doc_id": "f25cb767-4813-46e9-a13b-d065d7a43d4c"}, "504882d9-26b8-4125-a077-d28113018b43": {"doc_hash": "176bf266691d125c57429009b397a342fbe055b7f5792db8385967f14db01798", "ref_doc_id": "a4dbe7bd-c621-4d0f-9cf5-c606c5bdbeca"}, "fbf989f6-085d-4352-ac64-04c15e166159": {"doc_hash": "4bb1f6a04a7e87a1a5c142a373f3a46df27fc3dcd26c774a0016d94fa3330e68", "ref_doc_id": "a4dbe7bd-c621-4d0f-9cf5-c606c5bdbeca"}, "ba0721d6-b379-40da-b5c7-21d4f8114ee2": {"doc_hash": "fdbb757290cf06c95a59305c9218cacbb5cabe6dcef612f41351dfc3296ec976", "ref_doc_id": "f0c8b384-d483-40f3-a5e1-ef9403759988"}, "0f20fa45-56e7-430d-bc0e-da05eda1ccf2": {"doc_hash": "c5f86fa91c4499bc28293faa46deaf5b728e17ce1b229c0e073447944230caae", "ref_doc_id": "f0c8b384-d483-40f3-a5e1-ef9403759988"}, "ae7eefcb-b28a-47cd-9020-1da8e135f7e7": {"doc_hash": "a11e89a16a8dd63ecc38f446d7941c25c3bbe9b1694b14195b4b7fc1f896760c", "ref_doc_id": "cb84160c-0d61-4e72-acf1-3c958f0b6612"}, "3267a31f-55aa-4b04-8946-0c9db764b5fc": {"doc_hash": "f95fc5631f02f1aef436aa57155ed920d54ae4023dbdb03e34a6dceee43ebb76", "ref_doc_id": "cb84160c-0d61-4e72-acf1-3c958f0b6612"}, "d408db87-5aee-4b6e-b892-b0ca48dfcbfd": {"doc_hash": "00190a15751d67575625588c65b7da9173f25e4fd630d5fc38bafdb7b25055c2", "ref_doc_id": "e54a3d05-9f94-4788-ac98-306bacd5a7fd"}, "d8f6674b-4aaf-4b2d-9b01-3c0ffefbc8df": {"doc_hash": "f37e0ac8eda3cd488cf0aec25fbed5ae0a30ecca0a28034f5e6b527f875b9d48", "ref_doc_id": "e54a3d05-9f94-4788-ac98-306bacd5a7fd"}, "f6696a0f-ef63-44b3-a6aa-d66b4fa2725a": {"doc_hash": "a0bf7f9ee53ef3ed6fd00e65ed1996ae9135ea21239763394d18e125847d89b6", "ref_doc_id": "e54a3d05-9f94-4788-ac98-306bacd5a7fd"}, "95cdeea3-c80e-4d8d-8b2d-863cfedac0a1": {"doc_hash": "975c9c7abf64f5f3b114c17abc361f694883bd39c504e14e3af76a6c032596e1", "ref_doc_id": "42520c7e-d853-4163-8f41-f72a055edd7f"}, "da452dcb-6a0e-42c5-a458-2d7022588a9d": {"doc_hash": "d554fb51262b5e2ebd03a04182b02f14961ddfcfd090808702165f37eff20f88", "ref_doc_id": "42520c7e-d853-4163-8f41-f72a055edd7f"}, "5d258027-8e01-4a16-b858-ccf2ec4f0289": {"doc_hash": "fe2b403958ade9248ff6b8e27d727631434796c5fbe8e5ff139843c35f77a601", "ref_doc_id": "23ebbb42-b384-4b38-9f8e-02741508004f"}, "9567ec18-8c08-43ca-b1f7-e64d0a78ef1e": {"doc_hash": "42b68d5ce1cb051c3d23c85d38e8e945a970d73d629c95084b0605aad9224cb1", "ref_doc_id": "23ebbb42-b384-4b38-9f8e-02741508004f"}, "57eb4be8-d3a9-4074-b629-75604deb2111": {"doc_hash": "ab2950320611121582e6326b9dae5ecafb34853ab7a74aa83d0f8b69c83023ca", "ref_doc_id": "f8f7bf79-1e5a-4777-9a7c-1fabb237d5c3"}, "24275779-50fa-4a9a-9a35-d7f26265cb64": {"doc_hash": "1e75ce9d32637c19915c46c86320d5c609c343cfc3d256bf539ff0d46dac2240", "ref_doc_id": "f8f7bf79-1e5a-4777-9a7c-1fabb237d5c3"}, "65a7427c-4ad4-4484-a2c2-18dbc4a74793": {"doc_hash": "0702aed66901a763aed035d883f6339900a9137fe96641a2f5f91b509b91a096", "ref_doc_id": "00295924-da03-4122-81ec-e61b1e3d11b3"}, "95bca680-f877-4d5b-8e0c-04ee67549cd2": {"doc_hash": "3299ee350467a9b09529a5c0da18bcefb970f85cd116e0526520a25b981316b6", "ref_doc_id": "755014fd-2cc7-47a3-be4c-f4cfd8218a57"}, "938e1d0e-caa1-4f31-b361-1771db9508c6": {"doc_hash": "4d26c58daaa662f4267712e12e17e7a709e31656135463478239e1a9878caa0c", "ref_doc_id": "755014fd-2cc7-47a3-be4c-f4cfd8218a57"}}, "docstore/data": {"93cca829-42b2-46ed-8b90-c0ed8e48cc26": {"__data__": {"id_": "93cca829-42b2-46ed-8b90-c0ed8e48cc26", "embedding": null, "metadata": {"page_label": "1", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f25cb767-4813-46e9-a13b-d065d7a43d4c", "node_type": "4", "metadata": {"page_label": "1", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "436ea260cdf7a962e8528dc85c2e22ee8a870e1313d88116de41e2885557fbc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0c7cd52-3c8d-44f3-b3a7-a9143f3bc919", "node_type": "1", "metadata": {}, "hash": "6231c226b7cea277aee7317a5c4c3cf0d70c92d986ac0d7945ae46f1e8ef863f", "class_name": "RelatedNodeInfo"}}, "hash": "c843ccb4f8dc34692521cc3b39d708a782887e622ce82ec493d6d3b572a03222", "text": "DIALO GPT : Large-Scale Generative Pre-training\nfor Conversational Response Generation\nYizhe Zhang Siqi Sun Michel Galley Yen-Chun Chen\nChris Brockett Xiang Gao Jianfeng Gao Jingjing Liu Bill Dolan\nMicrosoft Corporation, Redmond, WA, USA\u2217\n{yizzhang,siqi.sun,mgalley,yenchen,chrisbkt,xiag,jfgao,jingjl,billdol }@microsoft.com\nAbstract\nWe present a large, tunable neural conversa-\ntional response generation model, D IALO GPT\n(dialogue generative pre-trained transformer).\nTrained on 147M conversation-like exchanges\nextracted from Reddit comment chains over\na period spanning from 2005 through 2017,\nDialoGPT extends the Hugging Face PyTorch\ntransformer to attain a performance close to\nhuman both in terms of automatic and human\nevaluation in single-turn dialogue settings. We\nshow that conversational systems that leverage\nDialoGPT generate more relevant, contentful\nand context-consistent responses than strong\nbaseline systems. The pre-trained model and\ntraining pipeline are publicly released to facil-\nitate research into neural response generation\nand the development of more intelligent open-\ndomain dialogue systems.\n1 Introduction\nWe introduce D IALO GPT, a tunable gigaword-\nscale neural network model for generation of con-\nversational reponses, trained on Reddit data.\nRecent advances in large-scale pre-training\nusing transformer-based architectures (Radford\net al., 2018; Devlin et al., 2019; Raffel et al., 2019)\nhave achieved great empirical success. OpenAI\u2019s\nGPT-2 (Radford et al., 2018), for example, has\ndemonstrated that transformer models trained on\nvery large datasets can capture long-term depen-\ndencies in textual data and generate text that is \ufb02u-\nent, lexically diverse, and rich in content. Such\nmodels have the capacity to capture textual data\nwith \ufb01ne granularity and produce output with a\nhigh-resolution that closely emulates real-world\ntext written by humans.\nDIALO GPT extends GPT-2 to address the chal-\nlenges of conversational neural response genera-\n\u2217A collaboration between Microsoft Research and Mi-\ncrosoft Dynamics 365 AI Research.tion. Neural response generation is a subcate-\ngory of text-generation that shares the objective of\ngenerating natural-looking text (distinct from any\ntraining instance) that is relevant to the prompt.\nModelling conversations, however, presents dis-\ntinct challenges in that human dialogue, which en-\ncapsulates the possibly competing goals of two\nparticipants, is intrinsically more diverse in the\nrange of potential responses (Li et al., 2016a;\nZhang et al., 2018; Gao et al., 2019a,b). It thus\nposes a greater one-to-many problem than is typ-\nical in other text generation tasks such as neural\nmachine translation, text summarization and para-\nphrasing. Human conversations are also generally\nmore informal, noisy, and, when in the form of\ntextual chat, often contain informal abbreviations\nor syntactic/lexical errors.\nMost open-domain neural response generation\nsystems suffer from content or style inconsis-\ntency (Li et al., 2016b; Zhang et al., 2019; Gao\net al., 2019c), lack of long-term contextual in-\nformation (Serban et al., 2017), and blandness\n(Li et al., 2016a; Zhang et al., 2018; Qin et al.,\n2019). While these issues can be alleviated by\nmodelling strategies speci\ufb01cally designed to boost\ninformation content, a transformer-based archi-\ntecture like GPT-2 (Radford et al., 2018), which\nuses a multi-layer self-attentive mechanism to al-\nlow fully-connected cross-attention to the full con-\ntext in a computationally ef\ufb01cient manner, seems\nlike a natural choice for exploring a more general\nsolution. Transformer models, for example, al-\nlow long-term dependency information to be bet-\nter be preserved across time (Radford et al., 2018),\nthereby improving content consistency.", "start_char_idx": 0, "end_char_idx": 3767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0c7cd52-3c8d-44f3-b3a7-a9143f3bc919": {"__data__": {"id_": "a0c7cd52-3c8d-44f3-b3a7-a9143f3bc919", "embedding": null, "metadata": {"page_label": "1", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f25cb767-4813-46e9-a13b-d065d7a43d4c", "node_type": "4", "metadata": {"page_label": "1", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "436ea260cdf7a962e8528dc85c2e22ee8a870e1313d88116de41e2885557fbc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93cca829-42b2-46ed-8b90-c0ed8e48cc26", "node_type": "1", "metadata": {"page_label": "1", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "c843ccb4f8dc34692521cc3b39d708a782887e622ce82ec493d6d3b572a03222", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "504882d9-26b8-4125-a077-d28113018b43", "node_type": "1", "metadata": {}, "hash": "176bf266691d125c57429009b397a342fbe055b7f5792db8385967f14db01798", "class_name": "RelatedNodeInfo"}}, "hash": "6231c226b7cea277aee7317a5c4c3cf0d70c92d986ac0d7945ae46f1e8ef863f", "text": "While these issues can be alleviated by\nmodelling strategies speci\ufb01cally designed to boost\ninformation content, a transformer-based archi-\ntecture like GPT-2 (Radford et al., 2018), which\nuses a multi-layer self-attentive mechanism to al-\nlow fully-connected cross-attention to the full con-\ntext in a computationally ef\ufb01cient manner, seems\nlike a natural choice for exploring a more general\nsolution. Transformer models, for example, al-\nlow long-term dependency information to be bet-\nter be preserved across time (Radford et al., 2018),\nthereby improving content consistency. They also\nhave higher model capacity due to their deep struc-\nture (up to 48 layers in GPT-2) and are more effec-\ntive in leveraging large-scale datasets (more than\n100 million training instances) than RNN-based\napproaches (Vaswani et al., 2017).arXiv:1911.00536v3  [cs.CL]  2 May 2020", "start_char_idx": 3189, "end_char_idx": 4053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "504882d9-26b8-4125-a077-d28113018b43": {"__data__": {"id_": "504882d9-26b8-4125-a077-d28113018b43", "embedding": null, "metadata": {"page_label": "2", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4dbe7bd-c621-4d0f-9cf5-c606c5bdbeca", "node_type": "4", "metadata": {"page_label": "2", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "65d59f99d02be5a5ef6142ea379b39e96b9d6148e5eb12501ef0c2e50f34dd7b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0c7cd52-3c8d-44f3-b3a7-a9143f3bc919", "node_type": "1", "metadata": {"page_label": "1", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "6231c226b7cea277aee7317a5c4c3cf0d70c92d986ac0d7945ae46f1e8ef863f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbf989f6-085d-4352-ac64-04c15e166159", "node_type": "1", "metadata": {}, "hash": "4bb1f6a04a7e87a1a5c142a373f3a46df27fc3dcd26c774a0016d94fa3330e68", "class_name": "RelatedNodeInfo"}}, "hash": "176bf266691d125c57429009b397a342fbe055b7f5792db8385967f14db01798", "text": "Like GPT-2, D IALO GPT is formulated as an\nautoregressive (AR) language model, and uses\nthe multi-layer transformer as model architecture.\nUnlike GPT-2, however, D IALO GPT is trained\non large-scale dialogue pairs/sessions extracted\nfrom Reddit discussion chains. Our assumption\nis that this should enable D IALO GPT to capture\nthe joint distribution of P(Target,Source )in con-\nversational \ufb02ow with \ufb01ner granularity. In prac-\ntice, this is what we observe: sentences gener-\nated by D IALO GPT are diverse and contain infor-\nmation speci\ufb01c to the source prompt, analogous\nwhat GPT-2 generates for continuous text. We\nhave evaluated the pre-trained model on a public\nbenchmark dataset (DSTC-7), and a new 6k multi-\nreference test dataset extracted from Reddit post-\nings. D IALO GPT achieves state-of-the-art results\nin both automatic and human evaluation, lifting\nperformance to near-human response quality.\nWe have released the source code and a\npre-trained model to facilitate future research.1.\nOur model can be easily leveraged and adapted\nto new dialogue datasets, especially datasets\nwith few training examples. The D IALO GPT\npackage also contains an open-source training\npipeline (data extraction/preparation and model\ntraining/evaluation) built upon the Huggingface\nPyTorch transformer (HuggingFace, 2019).2\n2 Dataset\nThe dataset is extracted from comment chains\nscraped from Reddit spanning from 2005 till 2017.\nReddit discussions can be naturally expanded as\ntree-structured reply chains, since a thread reply-\ning to one thread forms the root node of subse-\nquent threads. We extract each path from the root\nnode to the leaf node as a training instance con-\ntaining multiple turns of dialogue.\nWe \ufb01lter the data by removing the instances\nwhere (1) there is a URL in source or target, (2)\nwhere the target contains word repetitions of at\nleast three words, (3) where the response does\nnot contain at least one of the top-50 most fre-\nquent English words (e.g., \u201cthe\u201d, \u201cof\u201d, \u201ca\u201d), since\nthis probably indicates it might not be an English\nsentence, (4) where the response contains special\nmarkers such as \u201c[\u201d or \u201c]\u201d, as this could be markup\n1GitHub: https://github.com/microsoft/\nDialoGPT ; Blog: https://aka.ms/dialogpt\n2Our model is also available over Hugging face Trans-\nformers. https://huggingface.co/microsoft/\nDialoGPT-mediumlanguage, (5) where source and target sequences\ntogether are longer than 200 words, (6) where\nthe target contains offensive language, identi\ufb01ed\nby phrase matching against a large blocklist. We\nalso excluded a large number of subreddits that\nhad been identi\ufb01ed as likely to contain offensive\ncontent. In addition, we aggressively \ufb01ltered out\nblandness, e.g., removing instances where the re-\nsponses contained 90% of tri-grams that have been\nseen more than 1000 times. Often uninformative,\nsuch responses account for about 1% of the data.\nAfter \ufb01ltering, the dataset comprises 147,116,725\ndialogue instances, in total 1.8 billion words.\n3 Method\n3.1 Model Architecture\nWe trained our D IALO GPT model on the basis of\nthe GPT-2 (Radford et al., 2018) architecture.The\nGPT-2 transformer model adopts the generic trans-\nformer language model (Vaswani et al., 2017)\nand leverages a stack of masked multi-head self-\nattention layers to train on massive web-text data.\nThe text generated either from scratch or based\non a user-speci\ufb01c prompt is realistic-looking. The\nsuccess of GPT-2 demonstrates that a transformer\nlanguage model is able to characterize human lan-\nguage data distributions at a \ufb01ne-grained level,\npresumably due to large large model capacity and\nsuperior ef\ufb01ciency.\nOur model inherits from GPT-2 (Radford et al.,\n2018), a 12-to-48 layer transformer with layer nor-\nmalization, a initialization scheme that accounts\nfor model depth that we modi\ufb01ed, and byte pair\nencodings (Sennrich et al., 2016) for the tokenizer.", "start_char_idx": 0, "end_char_idx": 3868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbf989f6-085d-4352-ac64-04c15e166159": {"__data__": {"id_": "fbf989f6-085d-4352-ac64-04c15e166159", "embedding": null, "metadata": {"page_label": "2", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4dbe7bd-c621-4d0f-9cf5-c606c5bdbeca", "node_type": "4", "metadata": {"page_label": "2", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "65d59f99d02be5a5ef6142ea379b39e96b9d6148e5eb12501ef0c2e50f34dd7b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "504882d9-26b8-4125-a077-d28113018b43", "node_type": "1", "metadata": {"page_label": "2", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "176bf266691d125c57429009b397a342fbe055b7f5792db8385967f14db01798", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba0721d6-b379-40da-b5c7-21d4f8114ee2", "node_type": "1", "metadata": {}, "hash": "fdbb757290cf06c95a59305c9218cacbb5cabe6dcef612f41351dfc3296ec976", "class_name": "RelatedNodeInfo"}}, "hash": "4bb1f6a04a7e87a1a5c142a373f3a46df27fc3dcd26c774a0016d94fa3330e68", "text": "The text generated either from scratch or based\non a user-speci\ufb01c prompt is realistic-looking. The\nsuccess of GPT-2 demonstrates that a transformer\nlanguage model is able to characterize human lan-\nguage data distributions at a \ufb01ne-grained level,\npresumably due to large large model capacity and\nsuperior ef\ufb01ciency.\nOur model inherits from GPT-2 (Radford et al.,\n2018), a 12-to-48 layer transformer with layer nor-\nmalization, a initialization scheme that accounts\nfor model depth that we modi\ufb01ed, and byte pair\nencodings (Sennrich et al., 2016) for the tokenizer.\nWe follow the OpenAI GPT-2 to model a multi-\nturn dialogue session as a long text and frame the\ngeneration task as language modeling. We \ufb01rst\nconcatenate all dialog turns within a dialogue ses-\nsion into a long text x1,\u00b7\u00b7\u00b7,xN(Nis the se-\nquence length), ended by the end-of-text token.\nWe denote the source sentence (dialogue history)\nasS=x1,\u00b7\u00b7\u00b7,xmand target sentence (ground\ntruth response) as T=xm+1,\u00b7\u00b7\u00b7,xN, the condi-\ntional probability of P(T|S)can be written as the\nproduct of a series of conditional probabilities:\np(T|S) =N\u220f\nn=m+1p(xn|x1,\u00b7\u00b7\u00b7,xn\u22121)(1)\nFor a multi-turn dialogue session T1,\u00b7\u00b7\u00b7,TK, (1)\ncan be written as p(TK,\u00b7\u00b7\u00b7,T2|T1), which is", "start_char_idx": 3304, "end_char_idx": 4519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba0721d6-b379-40da-b5c7-21d4f8114ee2": {"__data__": {"id_": "ba0721d6-b379-40da-b5c7-21d4f8114ee2", "embedding": null, "metadata": {"page_label": "3", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0c8b384-d483-40f3-a5e1-ef9403759988", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "6f7378c1bdd90df040de6f913b6b67133178796971a12f18028d6666cbffd859", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbf989f6-085d-4352-ac64-04c15e166159", "node_type": "1", "metadata": {"page_label": "2", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "4bb1f6a04a7e87a1a5c142a373f3a46df27fc3dcd26c774a0016d94fa3330e68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f20fa45-56e7-430d-bc0e-da05eda1ccf2", "node_type": "1", "metadata": {}, "hash": "c5f86fa91c4499bc28293faa46deaf5b728e17ce1b229c0e073447944230caae", "class_name": "RelatedNodeInfo"}}, "hash": "fdbb757290cf06c95a59305c9218cacbb5cabe6dcef612f41351dfc3296ec976", "text": "essentially the product of conditional probabili-\nties ofp(Ti|T1,\u00b7\u00b7\u00b7,Ti\u22121). Consequently, opti-\nmizing a single objective p(TK,\u00b7\u00b7\u00b7,T2|T1)can\nbe perceived as optimizing all p(Ti|T1,\u00b7\u00b7\u00b7,Ti\u22121)\nsource-target pairs.\nOur implementation is based on the open-\nsource PyTorch-transformer repository.3\n3.2 Mutual Information Maximization\nOpen-domain text generation models are notori-\nous for generating bland, uninformative samples.\nTo address this problem, we implement a max-\nimum mutual information (MMI) scoring func-\ntion (Li et al., 2016a; Zhang et al., 2018). MMI\nemploys a pre-trained backward model to pre-\ndict source sentences from given responses, i.e.,\nP(Source|target ). We \ufb01rst generate a set of hy-\npotheses using top-K sampling. Then we use\nthe probability of P(Source|Hypothesis )to rerank\nall hypotheses. Intuitively, maximizing backward\nmodel likelihood penalizes the bland hypotheses,\nas frequent and repetitive hypotheses can be asso-\nciated with many possible queries, thus yielding a\nlower probability for any speci\ufb01c query.\nWe also attempted to optimize the reward R\u225c\nP(Source|Hypothesis )using a policy gradient\n(Williams, 1992) with a sample-averaged base-\nline, following Zhang et al. (2018). The valida-\ntion reward can be stably improved, but unlike the\ntraining under RNN architecture, we observed that\nreinforcement learning (RL) training easily con-\nverges to a degenerate locally-optimal solution,\nwhere the hypothesis simply repeats the source\nsentence (i.e., a parroting model) and mutual in-\nformation is maximized. We hypothesize that\ntransformers can become trapped in local optima\ndue to their strong model representation power.\nWe leave the investigation of regularized RL train-\ning to future work.\n4 Result\n4.1 Experimental Details\nWe trained 3 different sizes of the model with to-\ntal parameters of 117M, 345M and 762M respec-\ntively. The model speci\ufb01cation follows Radford\net al. (2018) (Table 1).\nOur model uses a vocabulary of 50,257 entries,\nand was trained on 16 Nvidia V100 machines with\n3https://github.com/huggingface/\npytorch-transformersModel LayersDemb B\n117M 12 768 128\n345M 24 1024 64\n762M 36 1280 32\nTable 1: Model con\ufb01gurations. \u201cB\u201d denotes batch size\nper GPU.\nNVLink. We used the Noam learning rate sched-\nuler with 16000 warm-up steps. The learning rate\nis selected based on validation loss. Each model\nis trained until there is no progress in validation\nloss. For small and medium models, we trained\nthe models for up to 5 epochs. For the large model\nwe trained for at most 3 epochs.\nSpeeding up training To accelerate the training\nprocess and accommodate GPU memory limita-\ntions, we \ufb01rst compress all training data into a\nlazy-loading database \ufb01le, so that data is loaded\nonly when needed (pre-fetching large chunks to\nreduce access frequency). We also leverage sepa-\nrate asynchronous data processes to scale the train-\ning. As a result, training time declines approxi-\nmately linearly w.r.t. the number of GPUs. We\nfurther employed a dynamic batching strategy to\ngroup conversations of similar lengths into the\nsame batch, thus increasing training throughput.\n4.2 DSTC-7 Dialogue Generation Challenge\nThe DSTC (Dialog System Technology Chal-\nlenges) 7 track (Galley et al., 2019) is an end-to-\nend conversational modeling task,4in which the\ngoal is to generate conversation responses that go\nbeyond chitchat by injecting information that is\ngrounded in external knowledge. This task is dis-\ntinct from what is commonly thought of as goal-\noriented, task-oriented, or task-completion dialogs\nin that there is no speci\ufb01c or prede\ufb01ned goal (e.g.,\nbooking a \ufb02ight, or reserving a table at a restau-\nrant).", "start_char_idx": 0, "end_char_idx": 3659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f20fa45-56e7-430d-bc0e-da05eda1ccf2": {"__data__": {"id_": "0f20fa45-56e7-430d-bc0e-da05eda1ccf2", "embedding": null, "metadata": {"page_label": "3", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0c8b384-d483-40f3-a5e1-ef9403759988", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "6f7378c1bdd90df040de6f913b6b67133178796971a12f18028d6666cbffd859", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba0721d6-b379-40da-b5c7-21d4f8114ee2", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "fdbb757290cf06c95a59305c9218cacbb5cabe6dcef612f41351dfc3296ec976", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae7eefcb-b28a-47cd-9020-1da8e135f7e7", "node_type": "1", "metadata": {}, "hash": "a11e89a16a8dd63ecc38f446d7941c25c3bbe9b1694b14195b4b7fc1f896760c", "class_name": "RelatedNodeInfo"}}, "hash": "c5f86fa91c4499bc28293faa46deaf5b728e17ce1b229c0e073447944230caae", "text": "the number of GPUs. We\nfurther employed a dynamic batching strategy to\ngroup conversations of similar lengths into the\nsame batch, thus increasing training throughput.\n4.2 DSTC-7 Dialogue Generation Challenge\nThe DSTC (Dialog System Technology Chal-\nlenges) 7 track (Galley et al., 2019) is an end-to-\nend conversational modeling task,4in which the\ngoal is to generate conversation responses that go\nbeyond chitchat by injecting information that is\ngrounded in external knowledge. This task is dis-\ntinct from what is commonly thought of as goal-\noriented, task-oriented, or task-completion dialogs\nin that there is no speci\ufb01c or prede\ufb01ned goal (e.g.,\nbooking a \ufb02ight, or reserving a table at a restau-\nrant). Instead, it targets human-like interactions\nwhere the underlying goal is often ill-de\ufb01ned or\nunknown in advance, of the kind seen in work and\nother productive environments (e.g., brainstorm-\ning meetings) where people share information.\nThe DSTC-7 test data contains conversation\nthreads from Reddit data. In order to create a\nmulti-reference test set, we utilized conversation\nsessions that contain 6 or more responses. Given\n4https://github.com/mgalley/\nDSTC7-End-to-End-Conversation-Modeling/\ntree/master/evaluation", "start_char_idx": 2950, "end_char_idx": 4178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae7eefcb-b28a-47cd-9020-1da8e135f7e7": {"__data__": {"id_": "ae7eefcb-b28a-47cd-9020-1da8e135f7e7", "embedding": null, "metadata": {"page_label": "4", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb84160c-0d61-4e72-acf1-3c958f0b6612", "node_type": "4", "metadata": {"page_label": "4", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "0711fe5fe93036f2d553e69dfd86c557373653f3285eb1b3b7038f39b0b9964e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f20fa45-56e7-430d-bc0e-da05eda1ccf2", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "c5f86fa91c4499bc28293faa46deaf5b728e17ce1b229c0e073447944230caae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3267a31f-55aa-4b04-8946-0c9db764b5fc", "node_type": "1", "metadata": {}, "hash": "f95fc5631f02f1aef436aa57155ed920d54ae4023dbdb03e34a6dceee43ebb76", "class_name": "RelatedNodeInfo"}}, "hash": "a11e89a16a8dd63ecc38f446d7941c25c3bbe9b1694b14195b4b7fc1f896760c", "text": "other \ufb01ltering criteria such as turn length, this\nyields a 5-reference test set of size 2208. (For each\ninstance, one of the 6 human responses is set aside\nto assess human performance on this task.) Note\nthat our training data is collected from a different\ntime span from the test set.\nWe performed automatic evaluation using stan-\ndard machine translation metrics, including BLEU\n(Papineni et al., 2002), METEOR (Lavie and\nAgarwal, 2007), and NIST (Doddington, 2002).\nNIST is a variant of BLEU that weights n-gram\nmatches by their information gain, i.e., it indi-\nrectly penalizes uninformative n-grams. We also\nuse Entropy (Zhang et al., 2018) and Dist-n (Li\net al., 2016a) to evaluate lexical diversity. More\ndetails are provided in Galley et al. (2019).\nWe compared D IALO GPT with our in-house\ncompetitive sequence-to-sequence model P ER-\nSONALITY CHAT based on (Li et al., 2016a) and\ntrained on Twitter data, which has been used in\nproduction as a Cognitive Service for Microsoft\nAzure.5Table 2 summarizes the automatic evalu-\nation results. D IALO GPT with 345M parameters\nand beam search achieved the highest automatic\nscore across most metrics. Scores for D IALO GPT\nwith 345M parameters are better across the board\nthan with 117M parameters. Beam search (with\nbeam width 10) dramatically improves BLEU and\nDIST scores, and marginally improves NIST and\nMETEOR. Note that our model is \ufb01ne-tuned on\nsource-target pairs, and does not leverage ground-\ning information from the DSTC training set. Pre-\nsumably, the model learns background informa-\ntion during pre-training and is unhindered by the\nlack of a grounding document.\nThe automatic scores of D IALO GPT are higher\nthan those for humans. This does not mean that\nthe generation is more \u201crealistic\u201d than human, but\nis probably attributable to the one-to-many nature\nof conversation. As illustrated in Figure 1, multi-\nple human responses (R1-R4) can correspond well\nto a source utterance. Without loss of general-\nity, suppose R1-R3 are the \u201cground truth\u201d refer-\nences that will be tested on, while R4 is the \u201cheld-\nout\u201d human response that serves to compute a \u201chu-\nman\u201d score. In semantic space, a generated re-\nsponseRgfrom a well-trained model will presum-\nably tend to lie in the vicinity the geometric center\n5Project P ERSONALITY CHAT: https:\n//docs.microsoft.com/en-us/\nazure/cognitive-services/\nproject-personality-chat/overview\nSource: I would like to report a break-in. R1: Was anything stolen?R2: Is anyone hurt or injured? R4: Is the perpetrator still inside?R3: I will send someone right away.Rg: When was this break-in?Figure 1: A generated response can surpass a human\nresponse in automatic metrics. Example responses are\nfrom Gupta et al. (2019)\nof all possible responses, because the training ob-\njective seeks to generate the most likely response.\nThis may be close to the geometric mean of all\ntraining instances, thus \u201caveraging out\u201d these in-\nstances. Consequently, a generated response Rg\nmight have a lower \u201csemantic distance\u201d (mani-\nfested in higher automatic scores like BLEU) from\nR1-R3 than the targeted human response R4.\n4.3 A New Reddit Multi-reference Dataset\nWe further evaluate D IALO GPT on a multi-\nreference test set with 6K examples. The results\nare shown in Table 3. We test our method on\ntwo settings: training from scratch and \ufb01ne-tuning\nusing GPT-2 as the pre-trained model. In both\nsettings, a larger model consistently outperforms\na smaller one. Comparing training from scratch\nto \ufb01ne-tuning from the pre-trained GPT-2 model,\nwhen applying to smaller model, using GPT-2\nmodel gives larger performance gains. Again, the\nbest system D IALO GPT (345M, w/ beam search)\nscores higher on BLEU than humans.", "start_char_idx": 0, "end_char_idx": 3703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3267a31f-55aa-4b04-8946-0c9db764b5fc": {"__data__": {"id_": "3267a31f-55aa-4b04-8946-0c9db764b5fc", "embedding": null, "metadata": {"page_label": "4", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb84160c-0d61-4e72-acf1-3c958f0b6612", "node_type": "4", "metadata": {"page_label": "4", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "0711fe5fe93036f2d553e69dfd86c557373653f3285eb1b3b7038f39b0b9964e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae7eefcb-b28a-47cd-9020-1da8e135f7e7", "node_type": "1", "metadata": {"page_label": "4", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "a11e89a16a8dd63ecc38f446d7941c25c3bbe9b1694b14195b4b7fc1f896760c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d408db87-5aee-4b6e-b892-b0ca48dfcbfd", "node_type": "1", "metadata": {}, "hash": "00190a15751d67575625588c65b7da9173f25e4fd630d5fc38bafdb7b25055c2", "class_name": "RelatedNodeInfo"}}, "hash": "f95fc5631f02f1aef436aa57155ed920d54ae4023dbdb03e34a6dceee43ebb76", "text": "4.3 A New Reddit Multi-reference Dataset\nWe further evaluate D IALO GPT on a multi-\nreference test set with 6K examples. The results\nare shown in Table 3. We test our method on\ntwo settings: training from scratch and \ufb01ne-tuning\nusing GPT-2 as the pre-trained model. In both\nsettings, a larger model consistently outperforms\na smaller one. Comparing training from scratch\nto \ufb01ne-tuning from the pre-trained GPT-2 model,\nwhen applying to smaller model, using GPT-2\nmodel gives larger performance gains. Again, the\nbest system D IALO GPT (345M, w/ beam search)\nscores higher on BLEU than humans. Larger mod-\nels trained from scratch (345M and 762M) per-\nform comparably to one \ufb01netuned on GPT-2.\n4.4 Re-ranking The Response Using MMI\nWe perform mutual information maximization as\ndescribed in Section 3.2. Speci\ufb01cally, we gener-\nate 16 samples for each input source sentence by\nusing top-K sampling ( K= 10 ) using the 345M\nmodel \ufb01ne-tuned from the GPT-2 medium model.\nThis is followed by a re-ranking step using a back-\nward model, which is also a 345M model \ufb01ne-\ntuned from the GPT-2 medium model. The re-\nsponse that yields lowest backward model loss is", "start_char_idx": 3111, "end_char_idx": 4264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d408db87-5aee-4b6e-b892-b0ca48dfcbfd": {"__data__": {"id_": "d408db87-5aee-4b6e-b892-b0ca48dfcbfd", "embedding": null, "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e54a3d05-9f94-4788-ac98-306bacd5a7fd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "0821f0a3aafc4e069257c105cf4c8a18a85ffb8bba7c705e926043bf9f80a906", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3267a31f-55aa-4b04-8946-0c9db764b5fc", "node_type": "1", "metadata": {"page_label": "4", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "f95fc5631f02f1aef436aa57155ed920d54ae4023dbdb03e34a6dceee43ebb76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8f6674b-4aaf-4b2d-9b01-3c0ffefbc8df", "node_type": "1", "metadata": {}, "hash": "f37e0ac8eda3cd488cf0aec25fbed5ae0a30ecca0a28034f5e6b527f875b9d48", "class_name": "RelatedNodeInfo"}}, "hash": "00190a15751d67575625588c65b7da9173f25e4fd630d5fc38bafdb7b25055c2", "text": "NIST BLEU METEOR Entropy Dist Avg Len\nMethod N-2 N-4 B-2 B-4 E-4 D-1 D-2\nPERSONALITY CHAT 0.19 0.20 10.44% 1.47% 5.42% 6.89 5.9% 16.4% 8.2\nTeam B 2.51 2.52 14.35% 1.83% 8.07% 9.03 10.9% 32.5% 15.1\nDIALO GPT (117M) 1.58 1.60 10.36% 2.02% 7.17% 6.94 6.2% 18.94% 13.0\nGPT(345M) 1.78 1.79 9.13% 1.06% 6.38% 9.72 11.9% 44.2% 14.7\nDIALO GPT (345M) 2.80 2.82 14.16% 2.31% 8.51% 10.08 9.1% 39.7% 16.9\nDIALO GPT (345M,Beam) 2.92 2.97 19.18 % 6.05% 9.29% 9.57 15.7% 51.0% 14.2\nHuman 2.62 2.65 12.35% 3.13% 8.31% 10.45 16.7% 67.0% 18.8\nTable 2: DSTC evaluation. \u201cTeam B\u201d is the winner system of the DSTC-7 challenge. \u201cBeam\u201d denotes beam\nsearch. \u201cHuman\u201d represents the held-out ground truth reference.", "start_char_idx": 0, "end_char_idx": 689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8f6674b-4aaf-4b2d-9b01-3c0ffefbc8df": {"__data__": {"id_": "d8f6674b-4aaf-4b2d-9b01-3c0ffefbc8df", "embedding": null, "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e54a3d05-9f94-4788-ac98-306bacd5a7fd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "0821f0a3aafc4e069257c105cf4c8a18a85ffb8bba7c705e926043bf9f80a906", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d408db87-5aee-4b6e-b892-b0ca48dfcbfd", "node_type": "1", "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "00190a15751d67575625588c65b7da9173f25e4fd630d5fc38bafdb7b25055c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6696a0f-ef63-44b3-a6aa-d66b4fa2725a", "node_type": "1", "metadata": {}, "hash": "a0bf7f9ee53ef3ed6fd00e65ed1996ae9135ea21239763394d18e125847d89b6", "class_name": "RelatedNodeInfo"}}, "hash": "f37e0ac8eda3cd488cf0aec25fbed5ae0a30ecca0a28034f5e6b527f875b9d48", "text": "\u201cTeam B\u201d is the winner system of the DSTC-7 challenge. \u201cBeam\u201d denotes beam\nsearch. \u201cHuman\u201d represents the held-out ground truth reference.\nNIST BLEU METEOR Entropy Dist Avg Len\nMethod N-2 N-4 B-2 B-4 E-4 D-1 D-2\nPERSONALITY CHAT 0.78 0.79 11.22% 1.95% 6.93% 8.37 5.8% 18.8% 8.12\nTraining from scratch:\nDIALO GPT (117M) 1.23 1.37 9.74% 1.77% 6.17% 7.11 5.3% 15.9% 9.41\nDIALO GPT (345M) 2.51 3.08 16.92% 4.59% 9.34% 9.03 6.7% 25.6% 11.16\nDIALO GPT (762M) 2.52 3.10 17.87% 5.19% 9.53% 9.32 7.5% 29.3% 10.72\nTraining from OpenAI GPT-2:\nDIALO GPT (117M) 2.39 2.41 10.54% 1.55% 7.53% 10.77 8.6% 39.9% 12.82\nDIALO GPT (345M) 3.00 3.06 16.96% 4.56% 9.81% 9.12 6.8% 26.3% 12.19\nDIALO GPT (345M, Beam) 3.4 3.5 21.76 % 7.92% 10.74% 10.48 12.38 % 48.74 % 11.34\nDIALO GPT (762M) 2.84 2.90 18.66% 5.25% 9.66% 9.72 7.76% 29.93% 11.19\nDIALO GPT (762M, Beam) 2.90 2.98 21.08% 7.57% 10.11% 10.06 11.62% 44.07% 10.68\nDIALO GPT (345M, MMI) 3.28 3.33 15.68% 3.94% 11.23 % 11.25 9.39% 45.55% 17.21\nHuman 3.41 4.25 17.90% 7.48% 10.64% 10.99 14.5% 63.0% 13.10\nTable 3: 6K Reddit multi-reference evaluation. \u201cBeam\u201d denotes beam search. \u201cHuman\u201d represents the held-out\nground truth reference.\nselected for evaluation. The results are summa-\nrized in the second to last row in Table 3. It can\nbe seen that compared with Greedy generation,\nMMI re-ranking produces more diverse responses\nwith higher NIST, METEOR and Entropy and Dist\nscores, but with a slight drop in BLEU.\n4.5 Generation Examples\nWe provide sample generated dialogues in Table 4\n(interactive chat) and Table 5 (a self-playing bot\nwith user prompt). The output is based on top-K\nsampling.6Interestingly, our model exhibits the\nability to address commonsense questions to some\nextent, presumably owing to the rich amount of\ninformation that can be learned from Reddit data.\nIn some cases, instead of giving the \u201cdesired\u201d an-\nswer, the system generates an alternative, reason-\nable answer. Our observations suggest that the\nsystem is able to deal with multi-turn generation\nbetter than an RNN counterpart and tends to be\n6Owing to occasional toxic outputs, we are unable to re-\nlease the generation code at this time.more consistent with respect to context (Table 5).7\nsource Response\nwho is the \ufb01rst president of the\nUnited States?George Washington\nwhat is the boiling point of\nwater?I think it\u2019s about 212 F.\nwhich one is bigger, sun or\nmoon?The sun .\nwhich animal has black and\nwhite stripes?A black and white\nstriped cat .", "start_char_idx": 551, "end_char_idx": 3013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6696a0f-ef63-44b3-a6aa-d66b4fa2725a": {"__data__": {"id_": "f6696a0f-ef63-44b3-a6aa-d66b4fa2725a", "embedding": null, "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e54a3d05-9f94-4788-ac98-306bacd5a7fd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "0821f0a3aafc4e069257c105cf4c8a18a85ffb8bba7c705e926043bf9f80a906", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8f6674b-4aaf-4b2d-9b01-3c0ffefbc8df", "node_type": "1", "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "f37e0ac8eda3cd488cf0aec25fbed5ae0a30ecca0a28034f5e6b527f875b9d48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95cdeea3-c80e-4d8d-8b2d-863cfedac0a1", "node_type": "1", "metadata": {}, "hash": "975c9c7abf64f5f3b114c17abc361f694883bd39c504e14e3af76a6c032596e1", "class_name": "RelatedNodeInfo"}}, "hash": "a0bf7f9ee53ef3ed6fd00e65ed1996ae9135ea21239763394d18e125847d89b6", "text": "In some cases, instead of giving the \u201cdesired\u201d an-\nswer, the system generates an alternative, reason-\nable answer. Our observations suggest that the\nsystem is able to deal with multi-turn generation\nbetter than an RNN counterpart and tends to be\n6Owing to occasional toxic outputs, we are unable to re-\nlease the generation code at this time.more consistent with respect to context (Table 5).7\nsource Response\nwho is the \ufb01rst president of the\nUnited States?George Washington\nwhat is the boiling point of\nwater?I think it\u2019s about 212 F.\nwhich one is bigger, sun or\nmoon?The sun .\nwhich animal has black and\nwhite stripes?A black and white\nstriped cat .\nTable 4: Addressing commonsense questions\n4.6 Human Evaluation\nHuman evaluations We evaluated 2000 ran-\ndomly sampled test sources from the Reddit 6K\ntest dataset using crowd-sourcing. Systems were\npaired and each pair of system outputs was ran-\ndomly presented to 3 judges, who ranked them\n7We provide a live invitation-only demonstration site for a\nconversational agents with toxicity controls and mutual infor-\nmation maximization features discussed in this paper. Check\nour GitHub repository for more information.", "start_char_idx": 2362, "end_char_idx": 3531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95cdeea3-c80e-4d8d-8b2d-863cfedac0a1": {"__data__": {"id_": "95cdeea3-c80e-4d8d-8b2d-863cfedac0a1", "embedding": null, "metadata": {"page_label": "6", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42520c7e-d853-4163-8f41-f72a055edd7f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "7c6f64c5e1ff7a1cda7f29d9a00839faeabcad1f83938186f40adcb21d8adc81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6696a0f-ef63-44b3-a6aa-d66b4fa2725a", "node_type": "1", "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "a0bf7f9ee53ef3ed6fd00e65ed1996ae9135ea21239763394d18e125847d89b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da452dcb-6a0e-42c5-a458-2d7022588a9d", "node_type": "1", "metadata": {}, "hash": "d554fb51262b5e2ebd03a04182b02f14961ddfcfd090808702165f37eff20f88", "class_name": "RelatedNodeInfo"}}, "hash": "975c9c7abf64f5f3b114c17abc361f694883bd39c504e14e3af76a6c032596e1", "text": "Role response\nUser Does money buy happiness?\nBot Depends how much money you spend on it .\nUser What is the best way to buy happiness ?\nBot You just have to be a millionaire by your early\n20s, then you can be happy .\nUser This is so dif\ufb01cult !\nBot You have no idea how hard it is to be a mil-\nlionaire and happy . There is a reason the rich\nhave a lot of money\nTable 5: An interactive example of multi-turn dialogue\nRole Response\nUser what is the meaning of life ?\nBot The meaning is to be with your family and\nfriends .\nBot I\u2019m going to guess : It means that your par-\nents and friends have loved you . That\u2019s the\nmeaning , I believe .\nBot I think the meaning is to make sure you\u2019re\nalive when you die . So you\u2019ll be with your\nfamily , friends , and yourself .\nBot It\u2019s not , but thanks for trying to help .\nTable 6: An example of multi-turn self-playing dia-\nlogue with user prompt\nfor relevance, informativeness and how human-\nlike the generating is using a 3-point Likert-like\nscale. Judges were required to pass a quali\ufb01ca-\ntion test, and a regime of spam detection was im-\nposed.8Overall judge preferences for relevance,\ninformativeness and human-likeness, presented as\nraw numbers and a percentage of the total, are\nshown in Table 7. A strong preference can be ob-\nserved for DialoGPT over PersonalityChat. Ta-\nble 7 also suggests that the \u201dvanilla\u201d DialoGPT\nmedium model may already be close to human\nresponse quality. Unexpectedly, we found that\njudges may prefer the MMI variant over human re-\nsponses, probably because of many of the true hu-\nman responses are erratic or idiosyncratic, or are\ntied to internet memes that happened to be unfa-\nmiliar to the judges.9(See Section 4.2 for the con-\nditions underlying this effect.) Further details, in-\ncluding a test of signi\ufb01cance and the human evalu-\nation template used, are provided in the Appendix.\n8We used held-out hand-vetted data from the human and\nPersonalityChat datasets to provide clear-cut cases for spam\nprevention and judge training examples. We suspect that this\nmay have helped bias the results towards the extremes.\n9For example, one judge protested that the internet meme\n\u201cI was today years old when I realized this.\u201d did not seem\nhuman-like.5 Related work\nThere are several open-sourced toolkits for large-\nscale pre-trained transformer models. Hugging-\nface Conv-AI transfer learning repository (Wolf\net al., 2019) contains the code for training con-\nversational AI systems with transfer learning\nbased on the GPT-2 transformer language model,\nwhich achieves the state-of-the-art performance\non ConvAI-2 dialogue competition. DLGnet\n(Olabiyi and Mueller, 2019) is a large transformer\nmodel trained on dialogue dataset and achieves\ngood performance in multi-turn dialogue genera-\ntion. AllenNLP (Gardner et al., 2018) is devel-\noped as a toolkit for many natural language pro-\ncessing tasks, including the large-scale pre-trained\nbi-LSTM sentence representation learning frame-\nwork ELMo (Peters et al., 2018). Texar (Hu et al.,\n2018) focuses on text generation including style\ntransferring and controllable generation. It in-\ncludes reinforcement learning capabilities along\nwith its sequence modelling tools. DeepPavlov\n(Burtsev et al., 2018) is a popular framework fo-\ncusing on task-oriented dialogue. This public\nrepository contains several demos and pre-trained\nmodels for question answering and sentiment clas-\nsi\ufb01cation. Icecaps (Shiv et al., 2019) is a re-\nsponse generation toolkit with techniques such\nas grounding on personalities or external knowl-\nedge and multi-task training. The ConvAI2 chal-\nlenge (Dinan et al., 2019) has a focus on person-\nalized conversations. ParlAI (Miller et al., 2017)\nis another library for developing task-oriented di-\nalogue systems. It contains pre-trained models for\nknowledge-grounded chatbot trained with crowd-\nsourced data. The Text-to-Text Transformer (Raf-\nfel et al., 2019) uni\ufb01es multiple text modeling\ntasks, and achieves the state-of-the-art results in\nvarious natural language generation and under-\nstanding benchmarks.", "start_char_idx": 0, "end_char_idx": 4055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da452dcb-6a0e-42c5-a458-2d7022588a9d": {"__data__": {"id_": "da452dcb-6a0e-42c5-a458-2d7022588a9d", "embedding": null, "metadata": {"page_label": "6", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42520c7e-d853-4163-8f41-f72a055edd7f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "7c6f64c5e1ff7a1cda7f29d9a00839faeabcad1f83938186f40adcb21d8adc81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95cdeea3-c80e-4d8d-8b2d-863cfedac0a1", "node_type": "1", "metadata": {"page_label": "6", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "975c9c7abf64f5f3b114c17abc361f694883bd39c504e14e3af76a6c032596e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d258027-8e01-4a16-b858-ccf2ec4f0289", "node_type": "1", "metadata": {}, "hash": "fe2b403958ade9248ff6b8e27d727631434796c5fbe8e5ff139843c35f77a601", "class_name": "RelatedNodeInfo"}}, "hash": "d554fb51262b5e2ebd03a04182b02f14961ddfcfd090808702165f37eff20f88", "text": "This public\nrepository contains several demos and pre-trained\nmodels for question answering and sentiment clas-\nsi\ufb01cation. Icecaps (Shiv et al., 2019) is a re-\nsponse generation toolkit with techniques such\nas grounding on personalities or external knowl-\nedge and multi-task training. The ConvAI2 chal-\nlenge (Dinan et al., 2019) has a focus on person-\nalized conversations. ParlAI (Miller et al., 2017)\nis another library for developing task-oriented di-\nalogue systems. It contains pre-trained models for\nknowledge-grounded chatbot trained with crowd-\nsourced data. The Text-to-Text Transformer (Raf-\nfel et al., 2019) uni\ufb01es multiple text modeling\ntasks, and achieves the state-of-the-art results in\nvarious natural language generation and under-\nstanding benchmarks.\n6 Limitations and risks\nDIALO GPT is released as a model only; the\nonus of decoder implementation resides with the\nuser. Despite our efforts to minimize the amount\nof overtly offensive data prior to training, D I-\nALOGPT retains the potential to generate output\nthat may trigger offense. Output may re\ufb02ect gen-\nder and other historical biases implicit in the data.\nResponses generated using this model may exhibit\na propensity to express agreement with proposi-\ntions that are unethical, biased or offensive (or the", "start_char_idx": 3284, "end_char_idx": 4571, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d258027-8e01-4a16-b858-ccf2ec4f0289": {"__data__": {"id_": "5d258027-8e01-4a16-b858-ccf2ec4f0289", "embedding": null, "metadata": {"page_label": "7", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23ebbb42-b384-4b38-9f8e-02741508004f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "dd06801e4d7fd8a0e312d30f1b47f0d261e509ece9269b0d40b389fde870bc10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da452dcb-6a0e-42c5-a458-2d7022588a9d", "node_type": "1", "metadata": {"page_label": "6", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "d554fb51262b5e2ebd03a04182b02f14961ddfcfd090808702165f37eff20f88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9567ec18-8c08-43ca-b1f7-e64d0a78ef1e", "node_type": "1", "metadata": {}, "hash": "42b68d5ce1cb051c3d23c85d38e8e945a970d73d629c95084b0605aad9224cb1", "class_name": "RelatedNodeInfo"}}, "hash": "fe2b403958ade9248ff6b8e27d727631434796c5fbe8e5ff139843c35f77a601", "text": "Relevance :A and B, which is more relevant and appropriate to the immediately preceding turn?\nSystem A Neutral System B\nDialoGPT (345M) 3281 ( 72%) 394 (9% ) 882 (19%) PersonalityChat ****\nDialoGPT (345M) 2379 (40%) 527 (9% ) 3094 ( 52%) DialoGPT (345M, w/ MMI) ****\nDialoGPT (345M) 3019 ( 50%) 581 (10%) 2400 (40%) DialoGPT (345M, Beam) ****\nDialoGPT (345M) 2726 ( 45%) 576 (10%) 2698 (45%) DialoGPT (762M)\nDialoGPT (345M) 2671 (45%) 513 (9% ) 2816 ( 47%) Human response\nDialoGPT (345M, w/ MMI) 2871 ( 48%) 522 (9%) 2607 (43%) Human response ***\nInformative :A and B, which is more contentful, interesting and informative?\nSystem A Neutral System B\nDialoGPT (345M) 3490 ( 77%) 206 (5%) 861 (19% ) PersonalityChat ****\nDialoGPT (345M) 2474 (41%) 257 (4%) 3269( 54%) DialoGPT (345M, w/ MMI) ****\nDialoGPT (345M) 3230 ( 54%) 362 (6%) 2408( 40%) DialoGPT (345M, Beam) *****\nDialoGPT (345M) 2856 ( 48%) 303 (5%) 2841( 47%) DialoGPT (762M)\nDialoGPT (345M) 2722 (45%) 234 (4%) 3044( 51%) Human response ****\nDialoGPT (345M, w/ MMI) 3011 ( 50%) 234 (4%) 2755( 46%) Human response **\nHuman-like :A and B, which is more likely to be generated by human rather than a chatbot?\nSystem A Neutral System B\nDialoGPT (345M) 3462 ( 76)% 196 (4%) 899 (20%) PersonalityChat ****\nDialoGPT (345M) 2478 (41)% 289 (5%) 3233 ( 54%) DialoGPT (345M, w/ MMI) ****\nDialoGPT (345M) 3233 ( 54)% 340 (6%) 2427 (40%) DialoGPT (345M, Beam) ****\nDialoGPT (345M) 2847 ( 47)% 321 (5%) 2832 (47%) DialoGPT (762M)\nDialoGPT (345M) 2716 (45)% 263 (4%) 3021 ( 50%) Human response ***\nDialoGPT (345M, w/ MMI) 2978 ( 50)% 241 (4%) 2781 (46%) Human response *\nTable 7: Results of Human Evaluation for relevance, informativeness and human-response possibility, showing\npreferences (%) for our model (DialoGPT) vis-a-vis its variants and real human responses. Distributions skew\ntowards DialoGPT with MMI, even when compared with human outputs. Numbers in bold indicate the preferred\nsystems. Statistically signi\ufb01cant results are indicated: * p \u22640.01, ** p\u22640.001, *** p\u22640.0001, **** p\u2264\n0.00001.\nreverse, disagreeing with otherwise ethical state-\nments). These are known issues in current state-\nof-the-art end-to-end conversation models trained\non large naturally-occurring datasets. A major\nmotive for releasing D IALO GPT is to enable re-\nsearchers to investigate these issues and develop\nmitigation strategies. In no case should inappro-\npriate content generated as a result of using D I-\nALOGPT be construed to re\ufb02ect the views or val-\nues of either the authors or Microsoft Corporation.\n7 Conclusion\nWe have released an open-domain pre-trained\nmodel, D IALO GPT, trained on massive real-world\nReddit dataset.", "start_char_idx": 0, "end_char_idx": 2667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9567ec18-8c08-43ca-b1f7-e64d0a78ef1e": {"__data__": {"id_": "9567ec18-8c08-43ca-b1f7-e64d0a78ef1e", "embedding": null, "metadata": {"page_label": "7", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23ebbb42-b384-4b38-9f8e-02741508004f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "dd06801e4d7fd8a0e312d30f1b47f0d261e509ece9269b0d40b389fde870bc10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d258027-8e01-4a16-b858-ccf2ec4f0289", "node_type": "1", "metadata": {"page_label": "7", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "fe2b403958ade9248ff6b8e27d727631434796c5fbe8e5ff139843c35f77a601", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57eb4be8-d3a9-4074-b629-75604deb2111", "node_type": "1", "metadata": {}, "hash": "ab2950320611121582e6326b9dae5ecafb34853ab7a74aa83d0f8b69c83023ca", "class_name": "RelatedNodeInfo"}}, "hash": "42b68d5ce1cb051c3d23c85d38e8e945a970d73d629c95084b0605aad9224cb1", "text": "Statistically signi\ufb01cant results are indicated: * p \u22640.01, ** p\u22640.001, *** p\u22640.0001, **** p\u2264\n0.00001.\nreverse, disagreeing with otherwise ethical state-\nments). These are known issues in current state-\nof-the-art end-to-end conversation models trained\non large naturally-occurring datasets. A major\nmotive for releasing D IALO GPT is to enable re-\nsearchers to investigate these issues and develop\nmitigation strategies. In no case should inappro-\npriate content generated as a result of using D I-\nALOGPT be construed to re\ufb02ect the views or val-\nues of either the authors or Microsoft Corporation.\n7 Conclusion\nWe have released an open-domain pre-trained\nmodel, D IALO GPT, trained on massive real-world\nReddit dataset. The package consists of a dis-\ntributed training pipeline and several pre-trained\nmodels that can be \ufb01ne-tuned to obtain a conver-\nsation model on a moderately-sized customized\ndataset in few hours. D IALO GPT is fully open-\nsourced and easy to deploy, allowing users to ex-tend the pre-trained conversational system to boot-\nstrap training using various datasets. It serves as a\nbuilding block to novel applications and method-\nologies. Detection and control of toxic output\nwill be a major focus of future investigation. We\nwill investigate leveraging reinforcement learning\nto further improve the relevance of the generated\nresponses and prevent the model from generating\negregious responses.\nAcknowledgements\nWe would like to thank Yu Wang, Vighnesh\nLeonardo Shiv, Chris Quirk, and the anonymous\nreviewers for their helpful discussions and com-\nments.\nReferences\nM. Burtsev, A. Seliverstov, R. Airapetyan,\nM. Arkhipov, D. Baymurzina, N. Bushkov,\nO. Gureenkova, T. Khakhulin, Y . Kuratov,", "start_char_idx": 1947, "end_char_idx": 3659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57eb4be8-d3a9-4074-b629-75604deb2111": {"__data__": {"id_": "57eb4be8-d3a9-4074-b629-75604deb2111", "embedding": null, "metadata": {"page_label": "8", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8f7bf79-1e5a-4777-9a7c-1fabb237d5c3", "node_type": "4", "metadata": {"page_label": "8", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "bbda9a6da5da84c4b996f11336b37d21f5909d33c357377fd7b413384ed65fa9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9567ec18-8c08-43ca-b1f7-e64d0a78ef1e", "node_type": "1", "metadata": {"page_label": "7", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "42b68d5ce1cb051c3d23c85d38e8e945a970d73d629c95084b0605aad9224cb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24275779-50fa-4a9a-9a35-d7f26265cb64", "node_type": "1", "metadata": {}, "hash": "1e75ce9d32637c19915c46c86320d5c609c343cfc3d256bf539ff0d46dac2240", "class_name": "RelatedNodeInfo"}}, "hash": "ab2950320611121582e6326b9dae5ecafb34853ab7a74aa83d0f8b69c83023ca", "text": "D. Kuznetsov, A. Litinsky, V . Logacheva, A. Lymar,\nV . Malykh, M. Petrov, V . Polulyakh, L. Pugachev,\nA. Sorokin, M. Vikhreva, and M. Zaynutdinov.\n2018. DeepPavlov: Open-source library for dia-\nlogue systems. In Proceedings of the 56th Annual\nMeeting of the Association for Computational\nLinguistics-System Demonstrations .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. NAACL 2019 .\nE. Dinan, V . Logacheva, V . Malykh, A. Miller, K. Shus-\nter, J. Urbanek, D. Kiela, A. Szlam, I. Serban,\nR. Lowe, S. Prabhumoye, A. W. Black, A. Rudnicky,\nJ. Williams, J. Pineau, M. Burtsev, and J. Weston.\n2019. The second conversational intelligence chal-\nlenge (ConvAI2).\nGeorge Doddington. 2002. Automatic evaluation\nof machine translation quality using n-gram co-\noccurrence statistics. In Proceedings of the second\ninternational conference on Human Language Tech-\nnology Research . Morgan Kaufmann Publishers Inc.\nMichel Galley, Chris Brockett, Xiang Gao, Jianfeng\nGao, and Bill Dolan. 2019. Grounded response gen-\neration task at DSTC7. In AAAI Dialog System\nTechnology Challenges Workshop .\nJ. Gao, M. Galley, and L. Li. 2019a. Neural approaches\nto conversational AI. Foundations and Trends in In-\nformation Retrieval .\nXiang Gao, Sungjin Lee, Yizhe Zhang, Chris Brockett,\nMichel Galley, Jianfeng Gao, and Bill Dolan. 2019b.\nJointly optimizing diversity and relevance in neural\nresponse generation. NAACL-HLT 2019 .\nXiang Gao, Yizhe Zhang, Sungjin Lee, Michel Gal-\nley, Chris Brockett, Jianfeng Gao, and Bill Dolan.\n2019c. Structuring latent spaces for stylized re-\nsponse generation. EMNLP-IJCNLP .\nM. Gardner, J. Grus, M. Neumann, O. Tafjord,\nP. Dasigi, N. F. Liu, M. Peters, M. Schmitz, and L. S.\nZettlemoyer. 2018. AllenNLP: A deep semantic nat-\nural language processing platform. In Proceedings\nof Workshop for NLP Open Source Software .\nPrakhar Gupta, Shikib Mehri, Tiancheng Zhao, Amy\nPavel, Maxine Eskenazi, and Jeffrey P Bigham.\n2019. Investigating evaluation of open-domain di-\nalogue systems with human generated multiple ref-\nerences. arXiv preprint arXiv:1907.10568 .\nZ. Hu, H. Shi, Z. Yang, B. Tan, T. Zhao, J. He,\nW. Wang, L. Qin, D. Wang, et al. 2018. Texar: A\nmodularized, versatile, and extensible toolkit for text\ngeneration. ACL.\nHuggingFace. 2019. PyTorch transformer reposi-\ntory.https://github.com/huggingface/\npytorch-transformers .Alon Lavie and Abhaya Agarwal. 2007. Meteor: An\nautomatic metric for mt evaluation with high levels\nof correlation with human judgments. In Proceed-\nings of the Second Workshop on Statistical Machine\nTranslation , pages 228\u2013231. Association for Com-\nputational Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng\nGao, and Bill Dolan. 2016a. A diversity-promoting\nobjective function for neural conversation models.\nNAACL .\nJiwei Li, Michel Galley, Chris Brockett, Georgios P\nSpithourakis, Jianfeng Gao, and Bill Dolan. 2016b.\nA persona-based neural conversation model. ACL.\nA. H. Miller, W. Feng, A. Fisch, J. Lu, D. Batra, A. Bor-\ndes, D. Parikh, and J. Weston. 2017.", "start_char_idx": 0, "end_char_idx": 3135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24275779-50fa-4a9a-9a35-d7f26265cb64": {"__data__": {"id_": "24275779-50fa-4a9a-9a35-d7f26265cb64", "embedding": null, "metadata": {"page_label": "8", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8f7bf79-1e5a-4777-9a7c-1fabb237d5c3", "node_type": "4", "metadata": {"page_label": "8", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "bbda9a6da5da84c4b996f11336b37d21f5909d33c357377fd7b413384ed65fa9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57eb4be8-d3a9-4074-b629-75604deb2111", "node_type": "1", "metadata": {"page_label": "8", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "ab2950320611121582e6326b9dae5ecafb34853ab7a74aa83d0f8b69c83023ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65a7427c-4ad4-4484-a2c2-18dbc4a74793", "node_type": "1", "metadata": {}, "hash": "0702aed66901a763aed035d883f6339900a9137fe96641a2f5f91b509b91a096", "class_name": "RelatedNodeInfo"}}, "hash": "1e75ce9d32637c19915c46c86320d5c609c343cfc3d256bf539ff0d46dac2240", "text": "2007. Meteor: An\nautomatic metric for mt evaluation with high levels\nof correlation with human judgments. In Proceed-\nings of the Second Workshop on Statistical Machine\nTranslation , pages 228\u2013231. Association for Com-\nputational Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng\nGao, and Bill Dolan. 2016a. A diversity-promoting\nobjective function for neural conversation models.\nNAACL .\nJiwei Li, Michel Galley, Chris Brockett, Georgios P\nSpithourakis, Jianfeng Gao, and Bill Dolan. 2016b.\nA persona-based neural conversation model. ACL.\nA. H. Miller, W. Feng, A. Fisch, J. Lu, D. Batra, A. Bor-\ndes, D. Parikh, and J. Weston. 2017. ParlAI: A di-\nalog research software platform. In Proceedings of\nthe 2017 EMNLP System Demonstration .\nOluwatobi Olabiyi and Erik T Mueller. 2019.\nMulti-turn dialogue response generation with\nautoregressive transformer models. arXiv\npreprint:1908.01841 .\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. ACL.\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner,\nC. Clark, K. Lee, and L. Zettlemoyer. 2018. Deep\ncontextualized word representations. NAACL .\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, Bill Dolan, Yejin Choi, and Jian-\nfeng Gao. 2019. Conversing by reading: Contentful\nneural conversation with on-demand machine read-\ning.ACL.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. 2018. Language models are unsuper-\nvised multitask learners. Technical report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uni\ufb01ed text-to-text trans-\nformer. arXiv preprint:1910.10683 .\nR. Sennrich, B. Haddow, and A. Birch. 2016. Neu-\nral machine translation of rare words with subword\nunits. ACL.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron Courville,\nand Yoshua Bengio. 2017. A hierarchical latent\nvariable encoder-decoder model for generating di-\nalogues. AAAI .\nVighnesh Leonardo Shiv, Chris Quirk, Anshuman Suri,\nXiang Gao, Khuram Shahid, Nithya Govindarajan,\nYizhe Zhang, Jianfeng Gao, Michel Galley, Chris\nBrockett, et al. 2019. Microsoft icecaps: An open-\nsource toolkit for conversation modeling. ACL.", "start_char_idx": 2488, "end_char_idx": 4850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65a7427c-4ad4-4484-a2c2-18dbc4a74793": {"__data__": {"id_": "65a7427c-4ad4-4484-a2c2-18dbc4a74793", "embedding": null, "metadata": {"page_label": "9", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00295924-da03-4122-81ec-e61b1e3d11b3", "node_type": "4", "metadata": {"page_label": "9", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "6781ba2615bc0ad6f3b3591e8d1a539d1206dade4685355673ca4fcacdc78a10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24275779-50fa-4a9a-9a35-d7f26265cb64", "node_type": "1", "metadata": {"page_label": "8", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "1e75ce9d32637c19915c46c86320d5c609c343cfc3d256bf539ff0d46dac2240", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95bca680-f877-4d5b-8e0c-04ee67549cd2", "node_type": "1", "metadata": {}, "hash": "3299ee350467a9b09529a5c0da18bcefb970f85cd116e0526520a25b981316b6", "class_name": "RelatedNodeInfo"}}, "hash": "0702aed66901a763aed035d883f6339900a9137fe96641a2f5f91b509b91a096", "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. NeurIPS .\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine learning .\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. TransferTransfo: A trans-\nfer learning approach for neural network based con-\nversational agents. CoRR , abs/1901.08149.\nYizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,\nXiujun Li, Chris Brockett, and Bill Dolan. 2018.\nGenerating informative and diverse conversational\nresponses via adversarial information maximization.\nNeurIPS .\nYizhe Zhang, Xiang Gao, Sungjin Lee, Chris Brockett,\nMichel Galley, Jianfeng Gao, and Bill Dolan. 2019.\nConsistent dialogue generation with self-supervised\nfeature learning. arXiv preprint:1903.05759 .", "start_char_idx": 0, "end_char_idx": 911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95bca680-f877-4d5b-8e0c-04ee67549cd2": {"__data__": {"id_": "95bca680-f877-4d5b-8e0c-04ee67549cd2", "embedding": null, "metadata": {"page_label": "10", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "755014fd-2cc7-47a3-be4c-f4cfd8218a57", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "1529feaed5e6a9c9a4a08cc5483c1562f1f1c3c14a0e124664031a664c20a323", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65a7427c-4ad4-4484-a2c2-18dbc4a74793", "node_type": "1", "metadata": {"page_label": "9", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "0702aed66901a763aed035d883f6339900a9137fe96641a2f5f91b509b91a096", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "938e1d0e-caa1-4f31-b361-1771db9508c6", "node_type": "1", "metadata": {}, "hash": "4d26c58daaa662f4267712e12e17e7a709e31656135463478239e1a9878caa0c", "class_name": "RelatedNodeInfo"}}, "hash": "3299ee350467a9b09529a5c0da18bcefb970f85cd116e0526520a25b981316b6", "text": "A Additional Details of Human Evaluation\nSigni\ufb01cance testing for the difference in means was performed using 10K bootstrap iterations. P-values\nare computed at \u03b1= 0.05. The results are provided in Table 8. The differences between 345M (2) and\n762M (6) models are not signi\ufb01cant. Notably also, the differences between 345M model (2) and human\nresponse (1) are not statistically signi\ufb01cant. The template for human evaluation is provided in Figure 2.\nSystem 1 System 2 Pairwise\nMean Std 95% CI Mean Std 95% CI Std 95% CI P-Value\n2 vs 1 Human-like 0.4527 0.0065 ( 0.4400, 0.4653 ) 0.5035 0.0065 ( 0.4909, 0.5162 ) 0.0127 (-0.0758, -0.0259 ) 0.0001\n2 vs 1 Informativeness 0.4537 0.0065 ( 0.4410, 0.4663 ) 0.5073 0.0064 ( 0.4948, 0.5199 ) 0.0127 (-0.0785, -0.0287 ) 0.0000\n2 vs 1 Relevance 0.4452 0.0064 (0.4326, 0.4577 ) 0.4693 0.0064 ( 0.4568, 0.4819 ) 0.0124 (-0.0485, 0.0002 ) 0.0552\n2 vs 3 Human-like 0.7597 0.0064 ( 0.7473, 0.7723 ) 0.1973 0.0059 ( 0.1858, 0.2089 ) 0.0117 ( 0.5392, 0.5852 ) 0.0000\n2 vs 3 Informativeness 0.7659 0.0063 ( 0.7536, 0.7783 ) 0.1889 0.0058 ( 0.1777, 0.2003 ) 0.0115 ( 0.5540, 0.5993 ) 0.0000\n2 vs 3 Relevance 0.7200 0.1935 ( 0.7070, 0.7333 ) 0.1935 0.0067 ( 0.7070, 0.7333 ) 0.0117 ( 0.5034, 0.5493 ) 0.0000\n2 vs 4 Human-like 0.4130 0.0063 ( 0.4005, 0.4253 ) 0.5388 0.0064 ( 0.5263, 0.5514 ) 0.0124 (-0.1504, -0.1016 ) 0.0000\n2 vs 4 Informativeness 0.4123 0.0063 ( 0.3999, 0.4246 ) 0.5448 0.0064 ( 0.5323, 0.5575 ) 0.0124 (-0.1570, -0.1082 ) 0.0000\n2 vs 4 Relevance 0.3965 0.0063 ( 0.3841, 0.4088 ) 0.5157 0.0064 ( 0.5031, 0.5281 ) 0.0122 (-0.1431, -0.0955 ) 0.0000\n2 vs 5 Human-like 0.5388 0.0064 ( 0.5263, 0.5513 ) 0.4045 0.0063 ( 0.3921, 0.4169 ) 0.0125 ( 0.1098, 0.1587 ) 0.0000\n2 vs 5 Informativeness 0.5383 0.0064 ( 0.5258, 0.5508 ) 0.4013 0.0063 ( 0.3890, 0.4137 ) 0.0124 ( 0.1127, 0.1611 ) 0.0000\n2 vs 5 Relevance 0.5032 0.0064 ( 0.4906,", "start_char_idx": 0, "end_char_idx": 1874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "938e1d0e-caa1-4f31-b361-1771db9508c6": {"__data__": {"id_": "938e1d0e-caa1-4f31-b361-1771db9508c6", "embedding": null, "metadata": {"page_label": "10", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "755014fd-2cc7-47a3-be4c-f4cfd8218a57", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "1529feaed5e6a9c9a4a08cc5483c1562f1f1c3c14a0e124664031a664c20a323", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95bca680-f877-4d5b-8e0c-04ee67549cd2", "node_type": "1", "metadata": {"page_label": "10", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}, "hash": "3299ee350467a9b09529a5c0da18bcefb970f85cd116e0526520a25b981316b6", "class_name": "RelatedNodeInfo"}}, "hash": "4d26c58daaa662f4267712e12e17e7a709e31656135463478239e1a9878caa0c", "text": "0122 (-0.1431, -0.0955 ) 0.0000\n2 vs 5 Human-like 0.5388 0.0064 ( 0.5263, 0.5513 ) 0.4045 0.0063 ( 0.3921, 0.4169 ) 0.0125 ( 0.1098, 0.1587 ) 0.0000\n2 vs 5 Informativeness 0.5383 0.0064 ( 0.5258, 0.5508 ) 0.4013 0.0063 ( 0.3890, 0.4137 ) 0.0124 ( 0.1127, 0.1611 ) 0.0000\n2 vs 5 Relevance 0.5032 0.0064 ( 0.4906, 0.5157 ) 0.4000 0.0063 ( 0.3876, 0.4124 ) 0.0122 ( 0.079, 0.127 ) 0.0000\n2 vs 6 Human-like 0.4745 0.0065 ( 0.4618, 0.4872 ) 0.4720 0.0064 ( 0.4596, 0.4846 ) 0.0125 (-0.0220, 0.0272 ) 0.8476\n2 vs 6 Informativeness 0.4760 0.0064 ( 0.4634, 0.4887 ) 0.4735 0.0064 ( 0.4610, 0.4861 ) 0.0126 (-0.0221, 0.0273 ) 0.8449\n2 vs 6 Relevance 0.4543 0.0065 ( 0.4417, 0.4671 ) 0.4497 0.0064 ( 0.4372, 0.4622 ) 0.0123 (-0.0193, 0.0289 ) 0.7066\n4 vs 1 Human-like 0.4963 0.0064 ( 0.4838, 0.5090 ) 0.4635 0.0065 ( 0.4508, 0.4762 ) 0.0127 ( 0.0081, 0.0578 ) 0.0094\n4 vs 1 Informativeness 0.5018 0.0064 ( 0.4894, 0.5144 ) 0.4592 0.0127 ( 0.0180, 0.0676 ) 0.0127 ( 0.0180, 0.0676 ) 0.0009\n4 vs 1 Relevance 0.4785 0.0064 ( 0.4660, 0.4911 ) 0.4345 0.0065 ( 0.4218, 0.4472 ) 0.0123 ( 0.0199, 0.0682 ) 0.0005\nTable 8: Human evaluation signi\ufb01cance test. Bold results represent differences that are NOT statistically signi\ufb01-\ncant. Notation: 1 - Human response; 2 - D IALO GPT 345M; 3 - PersonalityChat; 4 - D IALO GPT 345M w/ MMI;\n5 - D IALO GPT 345M Beam search; 6 - D IALO GPT 762M\nFigure 2: Human evaluation template", "start_char_idx": 1563, "end_char_idx": 2966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"f25cb767-4813-46e9-a13b-d065d7a43d4c": {"node_ids": ["93cca829-42b2-46ed-8b90-c0ed8e48cc26", "a0c7cd52-3c8d-44f3-b3a7-a9143f3bc919"], "metadata": {"page_label": "1", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}, "a4dbe7bd-c621-4d0f-9cf5-c606c5bdbeca": {"node_ids": ["504882d9-26b8-4125-a077-d28113018b43", "fbf989f6-085d-4352-ac64-04c15e166159"], "metadata": {"page_label": "2", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}, "f0c8b384-d483-40f3-a5e1-ef9403759988": {"node_ids": ["ba0721d6-b379-40da-b5c7-21d4f8114ee2", "0f20fa45-56e7-430d-bc0e-da05eda1ccf2"], "metadata": {"page_label": "3", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}, "cb84160c-0d61-4e72-acf1-3c958f0b6612": {"node_ids": ["ae7eefcb-b28a-47cd-9020-1da8e135f7e7", "3267a31f-55aa-4b04-8946-0c9db764b5fc"], "metadata": {"page_label": "4", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}, "e54a3d05-9f94-4788-ac98-306bacd5a7fd": {"node_ids": ["d408db87-5aee-4b6e-b892-b0ca48dfcbfd", "d8f6674b-4aaf-4b2d-9b01-3c0ffefbc8df", "f6696a0f-ef63-44b3-a6aa-d66b4fa2725a"], "metadata": {"page_label": "5", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}, "42520c7e-d853-4163-8f41-f72a055edd7f": {"node_ids": ["95cdeea3-c80e-4d8d-8b2d-863cfedac0a1", "da452dcb-6a0e-42c5-a458-2d7022588a9d"], "metadata": {"page_label": "6", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}, "23ebbb42-b384-4b38-9f8e-02741508004f": {"node_ids": ["5d258027-8e01-4a16-b858-ccf2ec4f0289", "9567ec18-8c08-43ca-b1f7-e64d0a78ef1e"], "metadata": {"page_label": "7", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}, "f8f7bf79-1e5a-4777-9a7c-1fabb237d5c3": {"node_ids": ["57eb4be8-d3a9-4074-b629-75604deb2111", "24275779-50fa-4a9a-9a35-d7f26265cb64"], "metadata": {"page_label": "8", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}, "00295924-da03-4122-81ec-e61b1e3d11b3": {"node_ids": ["65a7427c-4ad4-4484-a2c2-18dbc4a74793"], "metadata": {"page_label": "9", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}, "755014fd-2cc7-47a3-be4c-f4cfd8218a57": {"node_ids": ["95bca680-f877-4d5b-8e0c-04ee67549cd2", "938e1d0e-caa1-4f31-b361-1771db9508c6"], "metadata": {"page_label": "10", "file_name": "DIALOGPT.pdf", "file_path": "documents\\DIALOGPT.pdf", "file_type": "application/pdf", "file_size": 334623, "creation_date": "2023-12-17", "last_modified_date": "2023-10-03", "last_accessed_date": "2023-12-17"}}}}